{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_years(df):\n",
    "    \"\"\"\n",
    "    filters years 2013 and 2018 which we have to handle.\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'].dt.year.isin([2013, 2018])]\n",
    "    return df\n",
    "\n",
    "weather = pd.read_csv(\"weatherAUS.csv\")  # read csv data into pandas data frame\n",
    "weather = filter_years(weather)\n",
    "weather['Date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with missing target Variable \"RainTomorrow\"\n",
    "weather = weather[weather['RainTomorrow'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test showed that dropping at the threshholds used results in slightly better results.\n",
    "\n",
    "weather = weather[weather.columns[weather.isnull().mean() < 0.4]]\n",
    "weather = weather.loc[weather.isnull().mean(axis=1) < 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['RainToday'] = weather[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "weather['RainTomorrow'] = weather[\"RainTomorrow\"].replace({'No':0, 'Yes':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['Year'] = weather['Date'].dt.year  # get year\n",
    "weather['Month'] = weather['Date'].dt.month  # get month\n",
    "weather['Day'] = weather['Date'].dt.day  # get day\n",
    "weather.drop(labels=['Date', 'Location'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_season(month):\n",
    "    if month >= 9 and month <= 11:\n",
    "        return 'Spring'\n",
    "    if month == 12 or month <= 2:\n",
    "        return 'Summer'\n",
    "    if month >= 3 and month <= 5:\n",
    "        return 'Autumn'\n",
    "    if month >= 6 and month <= 8:\n",
    "        return 'Winter'\n",
    "    \n",
    "weather['Season'] = weather['Month'].apply(encode_season)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feate creation\n",
    "creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather['MinMaxDiff'] = weather['MaxTemp'] - weather['MinTemp']\n",
    "weather['PressureDiff'] = weather['Pressure3pm'] - weather['Pressure9am']\n",
    "weather['WindSpeedDiff'] = weather['Pressure3pm'] - weather['WindSpeed9am']\n",
    "weather['HumidityDiff'] = weather['Humidity3pm'] - weather['Humidity9am']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split\n",
    "**Important!** \n",
    "\n",
    "Before starting Feature Engineering one must split the dataset to ovoid test train leakage!\n",
    "All Decisions in Data Engineering must be made on the Train Set only! From here, we assume that we dont have any \n",
    "knowledge about the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(weather, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection\n",
    "**important**\n",
    "\n",
    "this must be done after splitting. Again: We do not know anything about the test data!\n",
    "\n",
    "-> Hat noch nix gebracht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = train.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = train.boxplot(column='WindGustSpeed')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindGustSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_lim = train['Rainfall'].quantile(0.995)\n",
    "\n",
    "train = train[train['Rainfall'] < higher_lim]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "higher_lim = train['WindGustSpeed'].quantile(0.995)\n",
    "\n",
    "train = train[train['WindGustSpeed'] < higher_lim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split part 2\n",
    "now we can split target variable and dependent variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.drop(['RainTomorrow'], axis=1)\n",
    "y_train = train['RainTomorrow']\n",
    "\n",
    "X_test = test.drop(['RainTomorrow'], axis=1)\n",
    "y_test = test['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute values the naiive approache without considering the locations or other stuff like season\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "    colums_containing_nan = dataset.columns[dataset.isnull().any()]\n",
    "    \n",
    "    numerical_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes != 'O']\n",
    "    categorial_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes == 'O']\n",
    "\n",
    "    for col in numerical_containing_nan:\n",
    "        col_median=X_train[col].median() #always use median from Train data ! Never impute based on Test Data ! we have to assume we dont know it.\n",
    "        dataset[col] = dataset[col].fillna(col_median) \n",
    "        \n",
    "    for col in categorial_containing_nan:\n",
    "        col_most_occuring = X_train[col].mode()[0]\n",
    "        dataset[col] = dataset[col].fillna(col_most_occuring)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Categorial Variables\n",
    "\n",
    "using One Hot Encoding..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply One Hot encoding\n",
    "\n",
    "for col in [\"WindGustDir\", \"WindDir9am\", \"WindDir3pm\", \"Season\"]:\n",
    "    encoded_columns = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "    X_train = X_train.join(encoded_columns).drop(col, axis=1)\n",
    "    \n",
    "    encoded_columns = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "    X_test = X_test.join(encoded_columns).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical features\n",
    "\n",
    "**Note !**\n",
    "This step is not required for decision trees. Still it is recommended to do. Why?\n",
    "1. We need it for Feature Selection \n",
    "2. If we change the tree to a regression or a NN, we need it.\n",
    "3. It doesnt do any harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cols = X_train.columns\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "X_test = pd.DataFrame(np.array(X_test), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "\n",
    "## The univariate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=10)\n",
    "\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=X_train.index, \n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "X_train[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multivariate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularisation strengh is set by regularization parameter C.\n",
    "# Note: the lower C, the higher the regularization\n",
    "logistic = LogisticRegression(C=0.05, penalty=\"l1\", solver='liblinear', max_iter=10000, random_state=7).fit(X_train, y_train)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X_train.index,\n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 EntscheidungsbÃ¤ume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "model.fit(X_train[selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "this is what we got from the beginnnig:\n",
    "    \n",
    "0.8435955056179776\n",
    "\n",
    "0.15640449438202247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null accuracy \n",
    "Whats the accuracy of a model, which just predicts the most common class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(5272/(5272+1403))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet 2 Neuronale Netze Aufgabe 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "test_dataset = test_dataset.shuffle(len(X_test)).batch(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = tf.keras.Sequential([\n",
    "    keras.layers.Dense(2, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.fit(train_dataset, \n",
    "       validation_steps=1,\n",
    "       validation_data=test_dataset,\n",
    "       epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
