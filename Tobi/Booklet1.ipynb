{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import category_encoders as ce\n",
    "import graphviz\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_years(df):\n",
    "    \"\"\"\n",
    "    filters years 2013 and 2018 which we have to handle.\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'].dt.year.isin([2013, 2018])]\n",
    "    return df\n",
    "\n",
    "weather = pd.read_csv(\"weatherAUS.csv\")  # read csv data into pandas data frame\n",
    "weather = filter_years(weather)\n",
    "weather['Date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten mit fehlender Zielvariable \"RainTomorrow\" rausschmeißen.\n",
    "# Man könnte diese Auch als Testvariablen nehmen, dann wären diese aber nicht zufällig ausgewählt...\n",
    "weather = weather[weather['RainTomorrow'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten, in denen mehr als 40% der Variablen fehlen rausscheißen.\n",
    "# Zeilen, in denen mehr als 50% der Variablen fehlen rausschmeißen.\n",
    "\n",
    "weather = weather[weather.columns[weather.isnull().mean() < 0.4]]\n",
    "weather = weather.loc[weather.isnull().mean(axis=1) < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feate creation\n",
    "creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['Year'] = weather['Date'].dt.year  # get year\n",
    "weather['Month'] = weather['Date'].dt.month  # get month\n",
    "weather['Day'] = weather['Date'].dt.day  # get day\n",
    "\n",
    "weather['MinMaxDiff'] = weather['MaxTemp'] - weather['MinTemp']\n",
    "weather['PressureDiff'] = weather['Pressure3pm'] - weather['Pressure9am']\n",
    "weather['WindSpeedDiff'] = weather['Pressure3pm'] - weather['WindSpeed9am']\n",
    "weather['HumidityDiff'] = weather['Humidity3pm'] - weather['Humidity9am']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Binning\n",
    "\n",
    "diskretisierung von Features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_season(month):\n",
    "    if month >= 9 and month <= 11:\n",
    "        return 'Spring'\n",
    "    if month == 12 or month <= 2:\n",
    "        return 'Summer'\n",
    "    if month >= 3 and month <= 5:\n",
    "        return 'Autumn'\n",
    "    if month >= 6 and month <= 8:\n",
    "        return 'Winter'\n",
    "    \n",
    "weather['Season'] = weather['Month'].apply(encode_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ist quasy \"Target Encoding\". Nur halt manuell...\n",
    "\n",
    "def encode_rainly_month(month):\n",
    "    rainy_month = [5,6, 7,8,11]\n",
    "    if month in rainy_month:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "weather['RainyMonth'] = weather['Month'].apply(encode_rainly_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split\n",
    "**Important!** \n",
    "\n",
    "Before starting Feature Engineering one must split the dataset to ovoid test train leakage!\n",
    "All Decisions in Data Engineering must be made on the Train Set only! From here, we assume that we dont have any \n",
    "knowledge about the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Zunächst wird noch nicht die Zielvariable \"abgespalten\"\n",
    "#Warum? Wenn die Zielvariable noch im gleichen DataFrame ist, kann man leichter Outlier rausschmeißen.\n",
    "train, test = train_test_split(weather, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection (Optional)\n",
    "**important**\n",
    "\n",
    "this must be done after splitting. Again: We do not know anything about the test data!\n",
    "\n",
    "**-> Verschlechtert das Ergebnis!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = train.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = train.boxplot(column='WindGustSpeed')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindGustSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "higher_lim = train['Rainfall'].quantile(0.995)\n",
    "train = train[train['Rainfall'] < higher_lim]\n",
    "higher_lim = train['WindGustSpeed'].quantile(0.995)\n",
    "train = train[train['WindGustSpeed'] < higher_lim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split part 2\n",
    "\n",
    "hier wird jetzt die Zielvariable abgespalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['RainTomorrow'], axis=1)\n",
    "y_train = train['RainTomorrow']\n",
    "\n",
    "X_test = test.drop(['RainTomorrow'], axis=1)\n",
    "y_test = test['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing Data (Univariat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute values the naiive approache without considering the locations or other stuff like season\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "    colums_containing_nan = dataset.columns[dataset.isnull().any()]\n",
    "    \n",
    "    numerical_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes != 'O']\n",
    "    categorial_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes == 'O']\n",
    "\n",
    "    for col in numerical_containing_nan:\n",
    "        col_median=X_train[col].median() #always use median from Train data ! Never impute based on Test Data ! we have to assume we dont know it.\n",
    "        dataset[col] = dataset[col].fillna(col_median) \n",
    "        \n",
    "    for col in categorial_containing_nan:\n",
    "        col_most_occuring = X_train[col].mode()[0]\n",
    "        dataset[col] = dataset[col].fillna(col_most_occuring)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['RainToday'] = X_train[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "X_test['RainToday'] = X_test[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "\n",
    "y_train = y_train.replace({'No':0, 'Yes':1})\n",
    "y_test = y_test.replace({'No':0, 'Yes':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding\n",
    "\n",
    "Beim target Encoding kodieren wir die Variable als Einfluss auf die Zielvariable. Wenn es also in Perth zu 20% geregnet hat, dann wird Perth mit 0.2 kodiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "cat_features=['Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "for feature in [cat_features]:\n",
    "    target_enc = ce.TargetEncoder(cols=feature)\n",
    "    target_enc.fit(X_train[feature], y_train)\n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    X_train = X_train.join(target_enc.transform(X_train[feature]).add_suffix('_target'))\n",
    "    X_test = X_test.join(target_enc.transform(X_test[feature]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply One Hot encoding\n",
    "\n",
    "for col in [\"Season\"]:\n",
    "    encoded_columns = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "    X_train = X_train.join(encoded_columns).drop(col, axis=1)\n",
    "    \n",
    "    encoded_columns = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "    X_test = X_test.join(encoded_columns).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Features\n",
    "\n",
    "Some features where transformed to other columns and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['Date', 'Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "X_train.drop(labels=columns_to_drop, axis=1, inplace=True)\n",
    "X_test.drop(labels=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical features\n",
    "\n",
    "**Note !**\n",
    "This step is not required for decision trees. Still it is recommended to do. Why?\n",
    "1. We need it for Feature Selection \n",
    "2. If we change the tree to a regression or a NN, we need it.\n",
    "3. It doesnt do any harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cols = X_train.columns\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "X_test = pd.DataFrame(np.array(X_test), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "\n",
    "## The univariate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# show correlation matrix\n",
    "train = X_train\n",
    "train[\"RainTomorrow\"] = y_train.values\n",
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# show correlation matrix as heatmap\n",
    "plt.imshow(train.corr(), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES_TO_SELECT = 5\n",
    "\n",
    "selector = SelectKBest(f_classif, k=NUM_FEATURES_TO_SELECT)\n",
    "\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=X_train.index, \n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "X_train[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multivariate way (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regularisation strengh is set by regularization parameter C.\n",
    "# Note: the lower C, the higher the regularization\n",
    "logistic = LogisticRegression(C=0.01, penalty=\"l1\", solver='liblinear', max_iter=10000, random_state=7).fit(X_train, y_train)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X_train.index,\n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy for a logistic regression model \n",
    "logistic = LogisticRegression().fit(X_train[selected_columns], y_train)\n",
    "\n",
    "test_predictions = logistic.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "model.fit(X_train[selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_predictions = model.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline\n",
    "\n",
    "this is what we got from the beginnnig:\n",
    "    \n",
    "0.8435955056179776\n",
    "\n",
    "0.15640449438202247"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model, ax=ax,\n",
    "              feature_names=selected_columns,\n",
    "              filled = True,\n",
    "              rounded = True,\n",
    "              precision =2,\n",
    "              fontsize = 10,\n",
    "               class_names=[\"no Rain\", \"Rain\"]\n",
    "              );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph = tree.export_graphviz(model, out_file=None, feature_names=selected_columns, class_names=[\"no Rain\", \"Rain\"])\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change max_depth\n",
    "model1 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "model1.fit(X_train[selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "test_predictions1 = model1.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions1))\n",
    "mean_absolute_error(y_test, test_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model1, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change min_impurity_increase\n",
    "model2 = tree.DecisionTreeClassifier(min_impurity_decrease=0.003)\n",
    "\n",
    "model2.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "# Tamara\n",
    "test_predictions2 = model2.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions2))\n",
    "mean_absolute_error(y_test, test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model2, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change max_leaf_nodes\n",
    "model3 = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "\n",
    "model3.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "# Tamara\n",
    "test_predictions3 = model3.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions3))\n",
    "mean_absolute_error(y_test, test_predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model3, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null accuracy \n",
    "Whats the accuracy of a model, which just predicts the most common class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(5272/(5272+1403))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet 2 Neuronale Netze Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearch to find the perfect Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wrap function around model creation so it can be used for grid search\n",
    "def build_model(n_hidden = 1, n_neurons=30, learning_rate=0.1, activation_function='relu', dropout_prop=0.25):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(2, activation='relu'))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=activation_function))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.add(keras.layers.Dropout(dropout_prop))\n",
    "    return model\n",
    "\n",
    "nn = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_hidden': [0,1,2,3,4],\n",
    "    'n_neurons': [1,3,5,10,20,50,100],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'activation_function': ['relu', 'sigmoid', 'elu'],\n",
    "    'dropout_prop': [0, 0.25, 0.5] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(nn, params, n_iter=20)\n",
    "random_search.fit(X_train.values, \n",
    "                  y_train.values,\n",
    "                  validation_data=(X_test.values, y_test.values),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=6)],\n",
    "                  batch_size=32,\n",
    "                  epochs=100\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the winner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dont need this anymore...\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "test_dataset = test_dataset.shuffle(len(X_test)).batch(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit(train_dataset\n",
    "       validation_steps=1,\n",
    "       validation_data=test_dataset,\n",
    "       batch_size=32,\n",
    "       epochs=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
