{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import category_encoders as ce\n",
    "import graphviz\n",
    "\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_years(df):\n",
    "    \"\"\"\n",
    "    filters years 2013 and 2018 which we have to handle.\n",
    "    \"\"\"\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'].dt.year.isin([2013, 2018])]\n",
    "    return df\n",
    "\n",
    "weather = pd.read_csv(\"weatherAUS.csv\")  # read csv data into pandas data frame\n",
    "weather = filter_years(weather)\n",
    "weather['Date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten mit fehlender Zielvariable \"RainTomorrow\" rausschmeißen.\n",
    "# Man könnte diese Auch als Testvariablen nehmen, dann wären diese aber nicht zufällig ausgewählt...\n",
    "weather = weather[weather['RainTomorrow'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten, in denen mehr als 40% der Variablen fehlen rausscheißen.\n",
    "# Zeilen, in denen mehr als 50% der Variablen fehlen rausschmeißen.\n",
    "\n",
    "weather = weather[weather.columns[weather.isnull().mean() < 0.4]]\n",
    "weather = weather.loc[weather.isnull().mean(axis=1) < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"RainTomorrow\", data=weather)\n",
    "\n",
    "ax.set_xlabel(\"RainTomorrow\", fontsize=18)\n",
    "ax.set_ylabel('Count', fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.savefig('distribution_target_variable.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feate creation\n",
    "creating new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['Year'] = weather['Date'].dt.year  # get year\n",
    "weather['Month'] = weather['Date'].dt.month  # get month\n",
    "weather['Day'] = weather['Date'].dt.day  # get day\n",
    "\n",
    "weather['MinMaxDiff'] = weather['MaxTemp'] - weather['MinTemp']\n",
    "weather['PressureDiff'] = weather['Pressure3pm'] - weather['Pressure9am']\n",
    "weather['WindSpeedDiff'] = weather['Pressure3pm'] - weather['WindSpeed9am']\n",
    "weather['HumidityDiff'] = weather['Humidity3pm'] - weather['Humidity9am']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Binning\n",
    "\n",
    "diskretisierung von Features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_season(month):\n",
    "    if month >= 9 and month <= 11:\n",
    "        return 'Spring'\n",
    "    if month == 12 or month <= 2:\n",
    "        return 'Summer'\n",
    "    if month >= 3 and month <= 5:\n",
    "        return 'Autumn'\n",
    "    if month >= 6 and month <= 8:\n",
    "        return 'Winter'\n",
    "    \n",
    "weather['Season'] = weather['Month'].apply(encode_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ist quasy \"Target Encoding\". Nur halt manuell...\n",
    "\n",
    "def encode_rainly_month(month):\n",
    "    rainy_month = [5,6, 7,8,11]\n",
    "    if month in rainy_month:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "weather['RainyMonth'] = weather['Month'].apply(encode_rainly_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test split\n",
    "**Important!** \n",
    "\n",
    "Before starting Feature Engineering one must split the dataset to ovoid test train leakage!\n",
    "All Decisions in Data Engineering must be made on the Train Set only! From here, we assume that we dont have any \n",
    "knowledge about the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Zunächst wird noch nicht die Zielvariable \"abgespalten\"\n",
    "#Warum? Wenn die Zielvariable noch im gleichen DataFrame ist, kann man leichter Outlier rausschmeißen.\n",
    "train, test = train_test_split(weather, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier detection (Optional)\n",
    "**important**\n",
    "\n",
    "this must be done after splitting. Again: We do not know anything about the test data!\n",
    "\n",
    "**-> Verschlechtert das Ergebnis!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = train.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = train.boxplot(column='WindGustSpeed')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindGustSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "higher_lim = train['Rainfall'].quantile(0.995)\n",
    "train = train[train['Rainfall'] < higher_lim]\n",
    "higher_lim = train['WindGustSpeed'].quantile(0.995)\n",
    "train = train[train['WindGustSpeed'] < higher_lim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split part 2\n",
    "\n",
    "hier wird jetzt die Zielvariable abgespalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['RainTomorrow'], axis=1)\n",
    "y_train = train['RainTomorrow']\n",
    "\n",
    "X_test = test.drop(['RainTomorrow'], axis=1)\n",
    "y_test = test['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Impute missing Data (Univariat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute values the naiive approache without considering the locations or other stuff like season\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "    colums_containing_nan = dataset.columns[dataset.isnull().any()]\n",
    "    \n",
    "    numerical_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes != 'O']\n",
    "    categorial_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes == 'O']\n",
    "\n",
    "    for col in numerical_containing_nan:\n",
    "        col_median=X_train[col].median() #always use median from Train data ! Never impute based on Test Data ! we have to assume we dont know it.\n",
    "        dataset[col] = dataset[col].fillna(col_median) \n",
    "        \n",
    "    for col in categorial_containing_nan:\n",
    "        col_most_occuring = X_train[col].mode()[0]\n",
    "        dataset[col] = dataset[col].fillna(col_most_occuring)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding Categorial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['RainToday'] = X_train[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "X_test['RainToday'] = X_test[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "\n",
    "y_train = y_train.replace({'No':0, 'Yes':1})\n",
    "y_test = y_test.replace({'No':0, 'Yes':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding\n",
    "\n",
    "Beim target Encoding kodieren wir die Variable als Einfluss auf die Zielvariable. Wenn es also in Perth zu 20% geregnet hat, dann wird Perth mit 0.2 kodiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "cat_features=['Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "for feature in [cat_features]:\n",
    "    target_enc = ce.TargetEncoder(cols=feature)\n",
    "    target_enc.fit(X_train[feature], y_train)\n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    X_train = X_train.join(target_enc.transform(X_train[feature]).add_suffix('_target'))\n",
    "    X_test = X_test.join(target_enc.transform(X_test[feature]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply One Hot encoding\n",
    "\n",
    "for col in [\"Season\"]:\n",
    "    encoded_columns = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "    X_train = X_train.join(encoded_columns).drop(col, axis=1)\n",
    "    \n",
    "    encoded_columns = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "    X_test = X_test.join(encoded_columns).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove Features\n",
    "\n",
    "Some features where transformed to other columns and can be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['Date', 'Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "X_train.drop(labels=columns_to_drop, axis=1, inplace=True)\n",
    "X_test.drop(labels=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scale numerical features\n",
    "\n",
    "**Note !**\n",
    "This step is not required for decision trees. Still it is recommended to do. Why?\n",
    "1. We need it for Feature Selection \n",
    "2. If we change the tree to a regression or a NN, we need it.\n",
    "3. It doesnt do any harm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cols = X_train.columns\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "X_test = pd.DataFrame(np.array(X_test), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "\n",
    "## The univariate way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# show correlation matrix\n",
    "train = X_train\n",
    "train[\"RainTomorrow\"] = y_train.values\n",
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# show correlation matrix as heatmap\n",
    "plt.imshow(train.corr(), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES_TO_SELECT = 5\n",
    "\n",
    "selector = SelectKBest(f_classif, k=NUM_FEATURES_TO_SELECT)\n",
    "\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=X_train.index, \n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "X_train[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The multivariate way (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regularisation strengh is set by regularization parameter C.\n",
    "# Note: the lower C, the higher the regularization\n",
    "logistic = LogisticRegression(C=0.01, penalty=\"l1\", solver='liblinear', max_iter=10000, random_state=7).fit(X_train, y_train)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X_train.index,\n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy for a logistic regression model \n",
    "logistic = LogisticRegression().fit(X_train[selected_columns], y_train)\n",
    "\n",
    "test_predictions = logistic.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(min_impurity_decrease=0.0001, max_depth=8)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = model.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7917541229385308\n",
    "0.20824587706146927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model, ax=ax,\n",
    "              #feature_names=selected_columns,\n",
    "              filled = True,\n",
    "              rounded = True,\n",
    "              precision =2,\n",
    "              fontsize = 10,\n",
    "               class_names=[\"no Rain\", \"Rain\"]\n",
    "              );\n",
    "\n",
    "plt.savefig('treeMinImpurityDecrease.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_graph = tree.export_graphviz(model, out_file=None, feature_names=selected_columns, class_names=[\"no Rain\", \"Rain\"])\n",
    "graphviz.Source(tree_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change max_depth\n",
    "model1 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "\n",
    "model1.fit(X_train[selected_columns], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "test_predictions1 = model1.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions1))\n",
    "mean_absolute_error(y_test, test_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model1, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change min_impurity_increase\n",
    "model2 = tree.DecisionTreeClassifier(min_impurity_decrease=0.003)\n",
    "\n",
    "model2.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "# Tamara\n",
    "test_predictions2 = model2.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions2))\n",
    "mean_absolute_error(y_test, test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model2, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# change max_leaf_nodes\n",
    "model3 = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "\n",
    "model3.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "# Tamara\n",
    "test_predictions3 = model3.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions3))\n",
    "mean_absolute_error(y_test, test_predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model3, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost complexity pruning\n",
    "Führe mit mind 3 Bäumen unterschiedlicher Tiefe aus Aufgabenteil c) ein Minimal Cost Complexity Pruning durch. Wie verändern dich die Bäume bei Variation des Prunings? Welche Auswirkung auf die Modellgüte hat das?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "model = tree.DecisionTreeClassifier()\n",
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "ccp_alphas.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "# take only subset of alphas to speed up further calculations\n",
    "ccp_alphas_part = ccp_alphas[[0, 68, 136, 204, 272, 340, 408, 476, 544, 612, 680, 748, 816, 884, 952, 1020, 1088, 1156, 1224, 1292, (ccp_alphas.size-20)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tamara\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tamara\n",
    "model = tree.DecisionTreeClassifier(ccp_alpha=0.002)\n",
    "model.fit(X_train[selected_columns], y_train)\n",
    "\n",
    "test_predictions = model.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas_part:\n",
    "    clf = tree.DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tamara\n",
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas_part, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas_part, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet 2 Neuronale Netze Aufgabe 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using RandomSearch to find the perfect Hyperparameter combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#wrap function around model creation so it can be used for grid search\n",
    "def build_model(n_hidden = 1, n_neurons=30, learning_rate=0.1, activation_function='relu', dropout_prop=0.25):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Dense(2, activation='relu'))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=activation_function))\n",
    "    model.add(keras.layers.Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=learning_rate),\n",
    "                  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                  metrics=['accuracy'])\n",
    "    model.add(keras.layers.Dropout(dropout_prop))\n",
    "    return model\n",
    "\n",
    "nn = keras.wrappers.scikit_learn.KerasClassifier(build_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'n_hidden': [0,1,2,3,4],\n",
    "    'n_neurons': [1,3,5,10,20,50,100],\n",
    "    'learning_rate': [0.1, 0.05, 0.01],\n",
    "    'activation_function': ['relu', 'sigmoid', 'elu'],\n",
    "    'dropout_prop': [0, 0.25, 0.5] \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search = RandomizedSearchCV(nn, params, n_iter=20)\n",
    "random_search.fit(X_train.values, \n",
    "                  y_train.values,\n",
    "                  validation_data=(X_test.values, y_test.values),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=6)],\n",
    "                  batch_size=32,\n",
    "                  epochs=100\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the winner Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dont need this anymore...\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values))\n",
    "train_dataset = train_dataset.shuffle(len(X_train)).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values))\n",
    "test_dataset = test_dataset.shuffle(len(X_test)).batch(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = keras.models.Sequential()\n",
    "nn.add(keras.layers.Dense(2, activation='elu'))\n",
    "for layer in range(4):\n",
    "        nn.add(keras.layers.Dense(3, activation='elu'))\n",
    "nn.add(keras.layers.Dense(1))\n",
    "    \n",
    "nn.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.1),\n",
    "           loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "           metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nn.fit(train_dataset,\n",
    "       validation_steps=1,\n",
    "       validation_data=test_dataset,\n",
    "       batch_size=32,\n",
    "       callbacks=[keras.callbacks.EarlyStopping(patience=6)],\n",
    "       epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet 3 Ensemblemethoden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Univariat Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_bagging = [{'n_estimators': [10, 50, 100]},\n",
    "                   {\"base_estimator__criterion\": [\"gini\", \"entropy\"]},\n",
    "                   {\"base_estimator__max_depth\": [None, 3,5,10, 15, 20, 25, 50]},\n",
    "                   {\"base_estimator__min_samples_split\": [2,5,10,20,30,40]},#The minimum number of samples required to split an internal node:\n",
    "                   {\"base_estimator__min_samples_leaf\": [1,2,5,10,20,40]},#The minimum number of samples required to be at a leaf node\n",
    "                   {\"base_estimator__min_weight_fraction_leaf\": [0, 0.2, 0.4, 0.5]},#The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. \n",
    "                   {\"base_estimator__max_features\": [5,10,15,20, 25, None]},\n",
    "                   {\"base_estimator__max_leaf_nodes\": [None, 2 ,10,100,150,200,300,500]},\n",
    "                   {\"base_estimator__min_impurity_decrease\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]},\n",
    "                   {\"base_estimator__min_impurity_split\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]} #dericated option\n",
    "                  ]\n",
    "\n",
    "params_adaboost = [{'n_estimators': [10, 50, 100]},\n",
    "                   {'learning_rate': [0.1, 0.5, 1]},\n",
    "                   {\"base_estimator__criterion\": [\"gini\", \"entropy\"]},\n",
    "                   #{\"base_estimator__max_depth\": [None, 3,5,10, 15, 20, 25, 50]},\n",
    "                   {\"base_estimator__min_samples_split\": [2,5,10,20,30,40]},#The minimum number of samples required to split an internal node:\n",
    "                   {\"base_estimator__min_samples_leaf\": [1,2,5,10,20,40]},#The minimum number of samples required to be at a leaf node\n",
    "                   {\"base_estimator__min_weight_fraction_leaf\": [0, 0.2, 0.4, 0.5]},#The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. \n",
    "                   {\"base_estimator__max_features\": [1,2,10,15,20, None]},\n",
    "                   {\"base_estimator__max_leaf_nodes\": [None,2,5,10,100,150,200]},\n",
    "                   {\"base_estimator__min_impurity_decrease\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]},\n",
    "                   {\"base_estimator__min_impurity_split\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]} #dericated option\n",
    "                  ]\n",
    "\n",
    "params_random_forest = [{'n_estimators': [10, 50, 100, 200, 500]},\n",
    "                   {\"criterion\": [\"gini\", \"entropy\"]},\n",
    "                   {\"max_depth\": [None, 3,5,10, 15, 20, 25, 50]},\n",
    "                   #{\"min_samples_split\": [2,5,10,20,30,40]},#The minimum number of samples required to split an internal node:\n",
    "                   {\"min_samples_leaf\": [1,2,5,10,20,40]},#The minimum number of samples required to be at a leaf node\n",
    "                   {\"min_weight_fraction_leaf\": [0, 0.2, 0.4, 0.5]},#The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. \n",
    "                   {\"max_features\": [5,10,15,20, 25, None]},\n",
    "                   {\"max_leaf_nodes\": [None, 2 ,10,100,150,200,300,500]},\n",
    "                   {\"min_impurity_decrease\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]},\n",
    "                   {\"min_impurity_split\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]} #dericated option\n",
    "                  ]\n",
    "\n",
    "params_tree = [{\"criterion\": [\"gini\", \"entropy\"]},\n",
    "                   #{\"max_depth\": [None, 3,5,10, 15, 20, 25, 50]},\n",
    "                   {\"min_samples_split\": [2,5,10,20,30,40]},#The minimum number of samples required to split an internal node:\n",
    "                   {\"min_samples_leaf\": [1,2,5,10,20,40]},#The minimum number of samples required to be at a leaf node\n",
    "                   {\"min_weight_fraction_leaf\": [0, 0.2, 0.4, 0.5]},#The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. \n",
    "                   {\"max_features\": [5,10,15,20, 25, None]},\n",
    "                   {\"max_leaf_nodes\": [None, 2 ,10,100,150,200,300,500]},\n",
    "                   {\"min_impurity_decrease\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]},\n",
    "                   {\"min_impurity_split\": [0.0, 0.1, 0.2, 0.5, 0.8, 1, 2, 3]} #dericated option\n",
    "                  ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through grid ourselfes for better interpretation of outputs.\n",
    "for param in params_random_forest:\n",
    "    \n",
    "    bagging_classifier = BaggingClassifier( \n",
    "        base_estimator=DecisionTreeClassifier(),\n",
    "        bootstrap=True, #replace training samples\n",
    "        n_jobs=-1, #use all available cores\n",
    "        random_state=42,\n",
    "  #      n_estimators=100\n",
    "    )\n",
    "    \n",
    "    bagging_gs = GridSearchCV(bagging_classifier, param, cv=10)\n",
    "    bagging_gs.fit(X_train, y_train)\n",
    "    print(param)\n",
    "    print(bagging_gs.best_params_)\n",
    "    print(bagging_gs.best_score_)\n",
    "    print(bagging_gs.cv_results_['mean_test_score'])\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for param in params_random_forest:\n",
    "    \n",
    "    adaboost_classifier = AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(\n",
    "        max_depth=5, \n",
    "        max_features=25,\n",
    "            \n",
    "            min_impurity_decrease=0.0005, \n",
    "            min_samples_split= 10\n",
    "        ),\n",
    "        random_state=42,\n",
    "        \n",
    "    )\n",
    "    \n",
    "    adaboost_gs = GridSearchCV(adaboost_classifier, param, cv=10)\n",
    "    adaboost_gs.fit(X_train, y_train)\n",
    "    print(param)\n",
    "    print(adaboost_gs.best_params_)\n",
    "    print(adaboost_gs.best_score_)\n",
    "    print(adaboost_gs.cv_results_['mean_test_score'])\n",
    "    print(\" \")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multivariat parameter space\n",
    "params_tree = [{\"max_depth\": [None,3,5,7],\n",
    "     \"min_samples_leaf\": [50, 100, 200]\n",
    "              }]\n",
    "\n",
    "params_random_forest = [{\n",
    "    \"base_estimator__max_depth\": [None,3, 5, 10],\n",
    "    \"base_estimator__min_samples_split\": [5, 10],\n",
    "    \"base_estimator__max_features\": [None, 5, 10, 25],\n",
    "    \"base_estimator__min_impurity_decrease\": [0.0001, 0.0005]\n",
    "              }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for param in params_random_forest:\n",
    "    \n",
    "    tree_classifier = DecisionTreeClassifier( \n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    \n",
    "    tree_gs = GridSearchCV(tree_classifier, params_random_forest, cv=10)\n",
    "    tree_gs.fit(X_train, y_train)\n",
    "    print(param)\n",
    "    print(tree_gs.best_params_)\n",
    "    print(tree_gs.best_score_)\n",
    "    print(tree_gs.cv_results_['mean_test_score'])\n",
    "    print(\" \")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for param in params_random_forest:\n",
    "    \n",
    "    random_forest_classifier = RandomForestClassifier(  \n",
    "        bootstrap=True, #replace training samples\n",
    "        random_state=42,\n",
    "        n_jobs=-1, #use all available cores,\n",
    "        n_estimators=100,\n",
    "    )\n",
    "\n",
    "    \n",
    "    random_forest_gs = GridSearchCV(random_forest_classifier, param, cv=10)\n",
    "    random_forest_gs.fit(X_train, y_train)\n",
    "    print(param)\n",
    "    print(random_forest_gs.best_params_)\n",
    "    print(random_forest_gs.best_score_)\n",
    "    print(random_forest_gs.cv_results_['mean_test_score'])\n",
    "    print(\" \")\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ist das hier nicht im Prinzip das Gleiche wie ein Random Forest mit splitter=\"random\"?\n",
    "start_time = time.time()\n",
    "\n",
    "bagging_classifier = BaggingClassifier( \n",
    "    base_estimator=DecisionTreeClassifier(\n",
    "     max_features=5, \n",
    "        min_impurity_decrease=0.0001, \n",
    "        min_samples_split=10\n",
    "    ),\n",
    "    bootstrap=True, #replace training samples\n",
    "    n_jobs=-1, #use all available cores\n",
    "    random_state=42,\n",
    "    n_estimators=100\n",
    ")\n",
    "bagging_classifier.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = bagging_classifier.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(\n",
    "        max_depth=20, \n",
    "        #max_features=25,\n",
    "            \n",
    "         #   min_impurity_decrease=0.0005, \n",
    "          #  min_samples_split= 10\n",
    "        ),\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = adaboost_classifier.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_classifier = RandomForestClassifier(  \n",
    "    bootstrap=True, #replace training samples\n",
    "    random_state=42,\n",
    "    n_jobs=-1, #use all available cores,\n",
    "    min_impurity_decrease=0.00005,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=100\n",
    ")\n",
    "\n",
    "random_forest_classifier.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = random_forest_classifier.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)\n",
    "\n",
    "# Feature importances with random forest\n",
    "for name, score in zip(X_train.columns, random_forest_classifier.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_classifier = DecisionTreeClassifier( \n",
    "    random_state=42,\n",
    "    min_impurity_decrease=0.0002,\n",
    "    min_samples_split=6\n",
    "    )\n",
    "\n",
    "tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = tree_classifier.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation of GridSearch results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = {\"max_depth\": [3,5,10, 15, 20, 25, 50,100]}\n",
    "#parameter = {\"min_impurity_decrease\": [0.0001, 0.001, 0.003, 0.005, 0.01, 0.05, 0.1, 0.2]}\n",
    "parameter_ = {\"base_estimator__max_depth\": [3,5,10, 15, 20, 25, 50,100]}\n",
    "\n",
    "## Tree\n",
    "tree_classifier = DecisionTreeClassifier( \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "tree_gs = GridSearchCV(tree_classifier, parameter, cv=10, return_train_score=True)\n",
    "tree_gs.fit(X_train, y_train)\n",
    "\n",
    "results_tree = tree_gs.cv_results_\n",
    "test_scores_tree = results_tree['mean_test_score']\n",
    "train_scores_tree = results_tree['mean_train_score']\n",
    "\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(  \n",
    "    bootstrap=True, #replace training samples\n",
    "    random_state=42,\n",
    "    n_jobs=-1, #use all available cores,\n",
    ")\n",
    "\n",
    "## Forest\n",
    "random_forest_gs = GridSearchCV(random_forest_classifier, parameter, cv=10, return_train_score=True)\n",
    "random_forest_gs.fit(X_train, y_train)\n",
    "\n",
    "results_forest = random_forest_gs.cv_results_\n",
    "test_scores_forest = results_forest['mean_test_score']\n",
    "train_scores_forest = results_forest['mean_train_score']\n",
    "\n",
    "## bagging\n",
    "bagging_classifier = BaggingClassifier( \n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    bootstrap=True, #replace training samples\n",
    "    n_jobs=-1, #use all available cores\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "bagging_gs = GridSearchCV(bagging_classifier, parameter_, cv=10, return_train_score=True)\n",
    "bagging_gs.fit(X_train, y_train)\n",
    "\n",
    "results_bagging = bagging_gs.cv_results_\n",
    "test_scores_bagging = results_bagging['mean_test_score']\n",
    "train_scores_bagging = results_bagging['mean_train_score']\n",
    "\n",
    "## Adaboost\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "adaboost_gs = GridSearchCV(adaboost_classifier, parameter_, cv=10, return_train_score=True)\n",
    "adaboost_gs.fit(X_train, y_train)\n",
    "\n",
    "results_adaboost = adaboost_gs.cv_results_\n",
    "test_scores_adaboost = results_adaboost['mean_test_score']\n",
    "train_scores_adaboost = results_adaboost['mean_train_score']\n",
    "\n",
    "\n",
    "X_axis = np.array(results_tree['param_max_depth'].data, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_index_tree = np.nonzero(results_tree['rank_test_score'] == 1)[0][0]\n",
    "best_score_tree = results_tree['mean_test_score'][best_index_tree]\n",
    "\n",
    "best_index_adaboost = np.nonzero(results_adaboost['rank_test_score'] == 1)[0][0]\n",
    "best_score_adaboost = results_adaboost['mean_test_score'][best_index_adaboost]\n",
    "\n",
    "best_index_forest = np.nonzero(results_forest['rank_test_score'] == 1)[0][0]\n",
    "best_score_forest = results_forest['mean_test_score'][best_index_forest]\n",
    "\n",
    "best_index_bagging = np.nonzero(results_bagging['rank_test_score'] == 1)[0][0]\n",
    "best_score_bagging = results_bagging['mean_test_score'][best_index_bagging]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "fig1, ax1 = plt.subplots()\n",
    "\n",
    "ax1.plot(X_axis, train_scores_tree, color='blue', alpha=0.7, linestyle='dashed' )\n",
    "ax1.plot(X_axis, test_scores_tree, color='blue')\n",
    "\n",
    "ax1.plot(X_axis, train_scores_forest, color = 'green', alpha=0.7, linestyle='dashed')\n",
    "ax1.plot(X_axis, test_scores_forest, color = 'green')\n",
    "\n",
    "ax1.plot(X_axis, train_scores_adaboost, color = 'red', alpha=0.7, linestyle='dashed')\n",
    "ax1.plot(X_axis, test_scores_adaboost, color = 'red')\n",
    "\n",
    "ax1.plot(X_axis, train_scores_bagging, color = 'purple', alpha=0.7, linestyle='dashed')\n",
    "ax1.plot(X_axis, test_scores_bagging, color = 'purple')\n",
    "\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xticks([3,5,10,20,50,100])\n",
    "ax1.get_xaxis().set_major_formatter(matplotlib.ticker.ScalarFormatter())\n",
    "ax1.set_xlabel(\"max_depth\", fontsize=14)\n",
    "ax1.set_ylabel('Accuracy', fontsize=14)\n",
    "plt.legend(['train tree','test tree', 'train forest', 'test forest','train adaboost', 'test adaboost', 'train bagging', 'test bagging'], fontsize=11)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=10)\n",
    "\n",
    "ax1.plot([X_axis[best_index_tree], ] * 2, [0, best_score_tree], linewidth=1,\n",
    "        linestyle='-.', color='blue', marker='x', markeredgewidth=3, ms=8)\n",
    "ax1.annotate(\"%0.3f\" % best_score_tree,\n",
    "            (X_axis[best_index_tree], best_score_tree + 0.005), backgroundcolor=\"w\", fontsize=12)\n",
    "\n",
    "ax1.plot([X_axis[best_index_adaboost], ] * 2, [0, best_score_adaboost], linewidth=1,\n",
    "        linestyle='-.', color='red', marker='x', markeredgewidth=3, ms=8)\n",
    "ax1.annotate(\"%0.3f\" % best_score_adaboost,\n",
    "            (X_axis[best_index_adaboost], best_score_adaboost + 0.005), backgroundcolor=\"w\", fontsize=12)\n",
    "\n",
    "ax1.plot([X_axis[best_index_forest], ] * 2, [0, best_score_forest], linewidth=1,\n",
    "        linestyle='-.', color='green', marker='x', markeredgewidth=3, ms=8)\n",
    "ax1.annotate(\"%0.3f\" % best_score_forest,\n",
    "            (X_axis[best_index_forest], best_score_forest + 0.005), backgroundcolor=\"w\", fontsize=12)\n",
    "\n",
    "ax1.plot([X_axis[best_index_bagging], ] * 2, [0, best_score_bagging], linewidth=1,\n",
    "        linestyle='-.', color='purple', marker='x', markeredgewidth=3, ms=8)\n",
    "ax1.annotate(\"%0.3f\" % best_score_bagging, \n",
    "            (X_axis[best_index_bagging], best_score_bagging + 0.005), backgroundcolor=\"w\", fontsize=12)\n",
    "\n",
    "ax1.set_ylim(0.75,1)\n",
    "\n",
    "plt.savefig('grid_search_max_depth.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Booklet Teil 4\n",
    "**Note**: Hab keine Ahnung was hier genau passiert. Aber es funktioniert vom Code her.\n",
    "### Parameters of SVM\n",
    "- C: Sets the margin. Kleines C führt zu einer große margin (inversely proportional). Große Margin regularisiert mehr, also tritt eher underfitting auf. Großes C overfittetet eher.\n",
    "- C kontrolliert den Bias-Varianze Tradeoff, indem es die tolerierte Missklassifikation und damit die Komplexität der Trennung bestimmt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### linear Kernel\n",
    "$\\langle x,x^{'}\\rangle$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear = SVC(kernel=\"linear\", C=1)\n",
    "svm_linear.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = svm_linear.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynominal Kernel\n",
    "$(\\gamma\\langle x,x^{'}\\rangle + r)^{d}$\n",
    "\n",
    "$d$ : degree\n",
    "\n",
    "$r$: coef0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_poly = SVC(kernel=\"poly\", degree=5,C=1)\n",
    "svm_poly.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = svm_poly.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "$exp(-\\gamma ||x-x^{'}||^{2})$\n",
    "\n",
    "$\\gamma$: gamma (>0)\n",
    "\n",
    "$\\gamma$ definiert den Einfluss der einzelenen Trainingsdaten\n",
    "Je größer $\\gamma$ ist, desto näher müssen andere Trainingsdatenpunkte sein, um einen Effekt zu haben\n",
    "$\\rightarrow \\gamma$ gibt invertiert den Einfluss-Radius der Datenpunkte an, die als Support Vectors bestimmt wurden (vom Modell). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf = SVC(kernel=\"rbf\", gamma=5, C=0.01)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = svm_rbf.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Kernel\n",
    "$tanh(\\gamma \\langle x,x^{'}\\rangle) + r)$\n",
    "\n",
    "$r$: coef0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_sigmoid = SVC(kernel=\"sigmoid\", C=0.01)\n",
    "svm_sigmoid.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = svm_sigmoid.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### precomputed Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gram_train = np.dot(X_train, X_train.T)\n",
    "gram_test = np.dot(X_test, X_train.T)\n",
    "\n",
    "svm_precomputed = SVC(kernel=\"precomputed\", C=0.01)\n",
    "\n",
    "svm_precomputed.fit(gram_train, y_train)\n",
    "\n",
    "test_predictions = svm_precomputed.predict(gram_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
