{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet Teil 1\n",
    "## Aufgabe 1: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def filter_years(df):\n",
    "    # filter Jahre 2013 and 2018 fuer unsere Gruppe\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df = df[df['Date'].dt.year.isin([2013, 2018])]\n",
    "    return df\n",
    "\n",
    "weather = pd.read_csv(\"weatherAUS.csv\")  # lese CSV Daten und schreibe sie in pandas Data Frame\n",
    "weather = filter_years(weather)\n",
    "weather['Date'].dt.year.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten mit fehlender Zielvariable \"RainTomorrow\" rausschmeißen.\n",
    "# Man könnte diese Auch als Testvariablen nehmen, dann wären diese aber nicht zufällig ausgewählt...\n",
    "weather = weather[weather['RainTomorrow'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Spalten, in denen mehr als 40% der Variablen fehlen rausscheißen.\n",
    "# Zeilen, in denen mehr als 50% der Variablen fehlen rausschmeißen.\n",
    "\n",
    "weather = weather[weather.columns[weather.isnull().mean() < 0.4]]\n",
    "weather = weather.loc[weather.isnull().mean(axis=1) < 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Null Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x=\"RainTomorrow\", data=weather)\n",
    "\n",
    "ax.set_xlabel(\"RainTomorrow\", fontsize=18)\n",
    "ax.set_ylabel('Count', fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=18)\n",
    "plt.yticks(fontsize=18)\n",
    "\n",
    "plt.savefig('distribution_target_variable.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feate creation\n",
    "Erstelle neue Merkmale anhand bereits bestehender Merkmale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weather['Year'] = weather['Date'].dt.year  # get year\n",
    "weather['Month'] = weather['Date'].dt.month  # get month\n",
    "weather['Day'] = weather['Date'].dt.day  # get day\n",
    "\n",
    "weather['MinMaxDiff'] = weather['MaxTemp'] - weather['MinTemp']\n",
    "weather['PressureDiff'] = weather['Pressure3pm'] - weather['Pressure9am']\n",
    "weather['WindSpeedDiff'] = weather['Pressure3pm'] - weather['WindSpeed9am']\n",
    "weather['HumidityDiff'] = weather['Humidity3pm'] - weather['Humidity9am']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Binning\n",
    "\n",
    "Diskretisiere bestehende Merkmale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def encode_season(month):\n",
    "    if month >= 9 and month <= 11:\n",
    "        return 'Spring'\n",
    "    if month == 12 or month <= 2:\n",
    "        return 'Summer'\n",
    "    if month >= 3 and month <= 5:\n",
    "        return 'Autumn'\n",
    "    if month >= 6 and month <= 8:\n",
    "        return 'Winter'\n",
    "    \n",
    "weather['Season'] = weather['Month'].apply(encode_season)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ist quasy \"Target Encoding\". Nur halt manuell...\n",
    "\n",
    "def encode_rainly_month(month):\n",
    "    rainy_month = [5,6, 7,8,11]\n",
    "    if month in rainy_month:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "weather['RainyMonth'] = weather['Month'].apply(encode_rainly_month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test split\n",
    "\n",
    "**Wichtig:** Bevor weiteres Feature Engineering betrieben wird, müssen die Daten in eine Test- und eine Trainingsmenge aufgeteilt werden, um *Test Train Leakage* zu vermeiden. Die Testmenge darf die Trainingsdaten nicht beeinflussen. Sie gilt als unbekannt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Zunächst wird noch nicht die Zielvariable \"abgespalten\"\n",
    "# Warum? Wenn die Zielvariable noch im gleichen DataFrame ist, kann man leichter Outlier rausschmeißen.\n",
    "train, test = train_test_split(weather, test_size=0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier detection (Optional)\n",
    "\n",
    "**-> Verschlechtert das Ergebnis!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "fig = train.boxplot(column='Rainfall')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('Rainfall')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "fig = train.boxplot(column='WindGustSpeed')\n",
    "fig.set_title('')\n",
    "fig.set_ylabel('WindGustSpeed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "higher_lim = train['Rainfall'].quantile(0.995)\n",
    "train = train[train['Rainfall'] < higher_lim]\n",
    "higher_lim = train['WindGustSpeed'].quantile(0.995)\n",
    "train = train[train['WindGustSpeed'] < higher_lim]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split part 2\n",
    "\n",
    "Hier wird jetzt die Zielvariable abgespalten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = train.drop(['RainTomorrow'], axis=1)\n",
    "y_train = train['RainTomorrow']\n",
    "\n",
    "X_test = test.drop(['RainTomorrow'], axis=1)\n",
    "y_test = test['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing Data (Univariat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Impute values the naiive approache without considering the locations or other stuff like season\n",
    "\n",
    "for dataset in [X_train, X_test]:\n",
    "\n",
    "    colums_containing_nan = dataset.columns[dataset.isnull().any()]\n",
    "    \n",
    "    numerical_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes != 'O']\n",
    "    categorial_containing_nan = [col for col in colums_containing_nan if dataset[col].dtypes == 'O']\n",
    "\n",
    "    for col in numerical_containing_nan:\n",
    "        col_median=X_train[col].median() #always use median from Train data ! Never impute based on Test Data ! we have to assume we dont know it.\n",
    "        dataset[col] = dataset[col].fillna(col_median) \n",
    "        \n",
    "    for col in categorial_containing_nan:\n",
    "        col_most_occuring = X_train[col].mode()[0]\n",
    "        dataset[col] = dataset[col].fillna(col_most_occuring)     \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Categorial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train['RainToday'] = X_train[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "X_test['RainToday'] = X_test[\"RainToday\"].replace({'No':0, 'Yes':1})\n",
    "\n",
    "y_train = y_train.replace({'No':0, 'Yes':1})\n",
    "y_test = y_test.replace({'No':0, 'Yes':1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target encoding\n",
    "\n",
    "Beim target Encoding kodieren wir die Variable als Einfluss auf die Zielvariable. Wenn es also in Perth zu 20% geregnet hat, dann wird Perth mit 0.2 kodiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the encoder\n",
    "cat_features=['Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "for feature in [cat_features]:\n",
    "    target_enc = ce.TargetEncoder(cols=feature)\n",
    "    target_enc.fit(X_train[feature], y_train)\n",
    "\n",
    "    # Transform the features, rename the columns with _target suffix, and join to dataframe\n",
    "    X_train = X_train.join(target_enc.transform(X_train[feature]).add_suffix('_target'))\n",
    "    X_test = X_test.join(target_enc.transform(X_test[feature]).add_suffix('_target'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# apply One Hot encoding\n",
    "\n",
    "for col in [\"Season\"]:\n",
    "    encoded_columns = pd.get_dummies(X_train[col], prefix=col, drop_first=True)\n",
    "    X_train = X_train.join(encoded_columns).drop(col, axis=1)\n",
    "    \n",
    "    encoded_columns = pd.get_dummies(X_test[col], prefix=col, drop_first=True)\n",
    "    X_test = X_test.join(encoded_columns).drop(col, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Features\n",
    "\n",
    "Einige Merkmale wurde in andere Merkmale transformiert, sodass die originalen Merkmale verworfen werden können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_drop = ['Date', 'Location','WindGustDir',\"WindDir9am\", \"WindDir3pm\"]\n",
    "X_train.drop(labels=columns_to_drop, axis=1, inplace=True)\n",
    "X_test.drop(labels=columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale numerical features\n",
    "\n",
    "**Bemerkung:**\n",
    "Dieser Schritt ist nicht notwendig für Entscheidungsbäume. Jedoch können diese auch mit skalierten Daten arbeiten, sodass an dieser Stelle bereits skaliert wird. Die Skalierung wird benötigt, falls eine Variablenselektion durchgeführt wird. Zudem arbeiten zum Beispiel neuronale Netze besser mit skalierten Daten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "cols = X_train.columns\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train = pd.DataFrame(X_train, columns=cols)\n",
    "X_test = pd.DataFrame(np.array(X_test), columns=cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection (Optional)\n",
    "**Bemerkung**: wurde im finalen Stand nicht ausgeführt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = weather.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Univariat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeige correlations matrix\n",
    "train = X_train\n",
    "train[\"RainTomorrow\"] = y_train.values\n",
    "train.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zeige heatmap\n",
    "plt.imshow(train.corr(), cmap='hot', interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NUM_FEATURES_TO_SELECT = 5\n",
    "\n",
    "selector = SelectKBest(f_classif, k=NUM_FEATURES_TO_SELECT)\n",
    "\n",
    "X_new = selector.fit_transform(X_train, y_train)\n",
    "\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), \n",
    "                                 index=X_train.index, \n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "\n",
    "X_train[selected_columns].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multivariat**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Regularisation strengh is set by regularization parameter C.\n",
    "# Note: the lower C, the higher the regularization\n",
    "logistic = LogisticRegression(C=0.01, penalty=\"l1\", solver='liblinear', max_iter=10000, random_state=7).fit(X_train, y_train)\n",
    "model = SelectFromModel(logistic, prefit=True)\n",
    "\n",
    "X_new = model.transform(X_train)\n",
    "\n",
    "selected_features = pd.DataFrame(model.inverse_transform(X_new), \n",
    "                                 index=X_train.index,\n",
    "                                 columns=X_train.columns)\n",
    "\n",
    "# Dropped columns have values of all 0s, keep other columns \n",
    "selected_columns = selected_features.columns[selected_features.var() != 0]\n",
    "selected_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting accuracy for a logistic regression model \n",
    "logistic = LogisticRegression().fit(X_train[selected_columns], y_train)\n",
    "\n",
    "test_predictions = logistic.predict(X_test[selected_columns]).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aufgabe 2 Entscheidungsbäume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Default-Einstellungen\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Errechne Genauigkeit und Testfehler\n",
    "test_predictions = model.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model, ax=ax,\n",
    "              feature_names=selected_columns,\n",
    "              filled = True,\n",
    "              rounded = True,\n",
    "              precision =2,\n",
    "              fontsize = 10,\n",
    "              class_names=[\"no Rain\", \"Rain\"]\n",
    "              );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depth Variation\n",
    "model1 = tree.DecisionTreeClassifier(max_depth=5)\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "test_predictions1 = model1.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions1))\n",
    "mean_absolute_error(y_test, test_predictions1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model1, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_impurity_increase variation\n",
    "model2 = tree.DecisionTreeClassifier(min_impurity_decrease=0.001)\n",
    "model2.fit(X_train, y_train)\n",
    "\n",
    "test_predictions2 = model2.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions2))\n",
    "mean_absolute_error(y_test, test_predictions2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model2, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion variation\n",
    "model3 = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n",
    "model3.fit(X_train, y_train)\n",
    "\n",
    "test_predictions3 = model3.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions3))\n",
    "mean_absolute_error(y_test, test_predictions3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model3, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "perm = PermutationImportance(model, random_state=1).fit(X_test, y_test)\n",
    "eli5.show_weights(perm, feature_names = X_test.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost-Complexity Pruning\n",
    "Führe mit mind. 3 Bäumen unterschiedlicher Tiefe aus Aufgabenteil c) ein Minimal Cost Complexity Pruning durch. Wie verändern dich die Bäume bei Variation des Prunings? Welche Auswirkung auf die Modellgüte hat das?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "path = model.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas, impurities = path.ccp_alphas, path.impurities\n",
    "ccp_alphas.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nur einen Teil der Ergebnisse nutzen zur Beschleunigung\n",
    "ccp_alphas_part = ccp_alphas[[0, 68, 136, 204, 272, 340, 408, 476, 544, 612, 680, 748, 816, 884, 952, 1020, 1088, 1156, 1224, 1292, (ccp_alphas.size-20)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(ccp_alphas[:-1], impurities[:-1], marker='o', drawstyle=\"steps-post\")\n",
    "ax.set_xlabel(\"effective alpha\")\n",
    "ax.set_ylabel(\"total impurity of leaves\")\n",
    "ax.set_title(\"Total Impurity vs effective alpha for training set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier(ccp_alpha=0.00025)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "test_predictions = model.predict(X_test).round().astype(int)\n",
    "print(accuracy_score(y_test, test_predictions))\n",
    "mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (20,10))\n",
    "tree.plot_tree(model, ax=ax,\n",
    "               feature_names=selected_columns,\n",
    "               class_names= [\"Rain\", \"no Rain\"],\n",
    "               filled = True,\n",
    "               rounded = True,\n",
    "               precision = 2,\n",
    "               fontsize = 10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = []\n",
    "for ccp_alpha in ccp_alphas_part:\n",
    "    clf = tree.DecisionTreeClassifier(ccp_alpha=ccp_alpha)\n",
    "    clf.fit(X_train, y_train)\n",
    "    clfs.append(clf)\n",
    "print(\"Number of nodes in the last tree is: {} with ccp_alpha: {}\".format(\n",
    "      clfs[-1].tree_.node_count, ccp_alphas[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = [clf.score(X_train, y_train) for clf in clfs]\n",
    "test_scores = [clf.score(X_test, y_test) for clf in clfs]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel(\"alpha\")\n",
    "ax.set_ylabel(\"accuracy\")\n",
    "ax.set_title(\"Accuracy vs alpha for training and testing sets\")\n",
    "ax.plot(ccp_alphas_part, train_scores, marker='o', label=\"train\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.plot(ccp_alphas_part, test_scores, marker='o', label=\"test\",\n",
    "        drawstyle=\"steps-post\")\n",
    "ax.legend()\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
