{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuronale Netze - Beispielprogramm\n",
    "### Erweiterung des Beispielprogramms um einen Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Preparations\n",
    "#### 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Generating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "n          = 1000\n",
    "data_seed  = 1337\n",
    "split_seed = 42\n",
    "test_size  = 0.25\n",
    "\n",
    "np.random.seed(data_seed)\n",
    "X, y = sklearn.datasets.make_moons(n, noise=0.25)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=split_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Implementierung\n",
    "#### 2.1 - Aktivierungsfunktion (activation function)\n",
    "Als Aktivierungsfunktion des Output-Layers wird die logistische Funktion \n",
    "\n",
    "$\\displaystyle \\sigma\\colon \\,\\mathbb{R}\\to \\left(0,1\\right), \\;z \\mapsto\\frac{1}{1+\\exp(-z)}$ \n",
    "\n",
    "verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7310585786300049\n",
      "0.19661193324148185\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z, derivation = False):\n",
    "    if derivation:\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "value = 1\n",
    "print(sigmoid(value))\n",
    "print(sigmoid(value,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Kostenfunktion (cost function)\n",
    "Als Kostenfunktion wird folgende quadratische Fehlerfunktion verwendet:\n",
    "\n",
    "$\\displaystyle E(y,\\widehat{\\pi}) = \\frac{1}{2}\\sum_{k=1}^{n}\\left(y_k-\\widehat{\\pi_k}\\right)^2$\n",
    "\n",
    "wobei $y=\\left(y_1,\\dots,y_n\\right)^T$ und $\\widehat{\\pi}=\\left(\\widehat{\\pi_1},\\dots,\\widehat{\\pi_n}\\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet Teil 2\n",
    "**hier startet unsere Implementierung:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # set random seed to get reproducable results\n",
    "        np.random.seed(5)\n",
    "        \n",
    "        # Glorot initialisation\n",
    "        \n",
    "        self.W_input_hidden = np.random.normal(0.0, \n",
    "                                               1/((self.input_nodes*self.hidden_nodes+self.hidden_nodes)/2), \n",
    "                                               (self.hidden_nodes, self.input_nodes))\n",
    "        \n",
    "        self.W_hidden_output = np.random.normal(0.0, \n",
    "                                               1/((self.hidden_nodes*self.output_nodes+self.output_nodes)/2), \n",
    "                                               (self.output_nodes, self.hidden_nodes))\n",
    "        \n",
    "        self.bias_input = np.zeros((self.hidden_nodes, 1))\n",
    "        \n",
    "        self.bias_hidden = np.zeros((self.output_nodes, 1))\n",
    "    \n",
    "    def calculate_loss(self, inputs,labels):\n",
    "        predictions = self.predict(inputs)\n",
    "\n",
    "        # Berechnung des Kostenfunktionswertes\n",
    "        cost = np.power(predictions-labels,2)\n",
    "        cost = np.sum(cost)/2\n",
    "\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def fit(self, inputs, label):\n",
    "        label = np.array(label, ndmin=2).T\n",
    "        output, hidden_outputs = self.predict(inputs, backprop=True)\n",
    "        \n",
    "        # Update weights from hidden to output layer\n",
    "        output_error = output - label  #getting this from the derived squared error loss function\n",
    "        # note that \"output_error*outputs*(1.0-outputs)\" comes from the derivate of sigmoid acrivation.\n",
    "        gradient_weights_hidden_output = np.dot((output_error*output*(1.0-output)), np.transpose(hidden_outputs))\n",
    "        self.W_hidden_output += -self.learning_rate * gradient_weights_hidden_output\n",
    "        \n",
    "        # update weights of bias_hidden\n",
    "        gradient_weights_bias_hidden = output_error*output*(1.0-output)\n",
    "        self.bias_hidden += - self.learning_rate * gradient_weights_bias_hidden\n",
    "        \n",
    "        # Update weights from input to hidden layer\n",
    "        # first \"propagate\" the errors back to the hidden layer.\n",
    "        hidden_errors = np.dot(self.W_hidden_output.T, (output_error*output*(1.0-output)))\n",
    "        \n",
    "        # this step is the same as from the previous update. just one layer closer to the input.\n",
    "        gradient_weights_input_hidden = np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)),\n",
    "                                              np.array(inputs, ndmin=2))\n",
    "        self.W_input_hidden += -self.learning_rate * gradient_weights_input_hidden\n",
    "        \n",
    "        # update weights of bias_input\n",
    "        gradient_weights_bias_input = (hidden_errors * hidden_outputs * (1.0 - hidden_outputs))\n",
    "        self.bias_input = self.bias_input - self.learning_rate * gradient_weights_bias_input\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def predict(self, inputs, backprop=False):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin=2).T\n",
    "        \n",
    "        # feedforward from input layer to hidden layer\n",
    "        hidden_inputs = np.dot(self.W_input_hidden, inputs) + self.bias_input\n",
    "        hidden_outputs = sigmoid(hidden_inputs)\n",
    "        \n",
    "        # feedforward from hidden layer to output layer\n",
    "        output_inputs = np.dot(self.W_hidden_output, hidden_outputs) + self.bias_hidden\n",
    "        output_outputs = sigmoid(output_inputs)\n",
    "        \n",
    "        if backprop == True:\n",
    "            # returning hidden outputs which we need in case of backpropagation\n",
    "            return (output_outputs, hidden_outputs)\n",
    "        \n",
    "        return output_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create nn\n",
    "nn = NeuralNet(input_nodes=2, hidden_nodes=100, output_nodes=1, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# train nn\n",
    "for epoch in range(15):\n",
    "    for sample, sample_label, in zip(X_train, y_train):\n",
    "        nn.fit(sample, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.84704733671989"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.calculate_loss(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the accuracy\n",
    "predictions = nn.predict(X_test)\n",
    "rounded_predictions = np.where(predictions > 0.5,1,0)\n",
    "rounded_predictions[0]\n",
    "\n",
    "accuracy = (rounded_predictions[0] == y_test).mean()\n",
    "accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
