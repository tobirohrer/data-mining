{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining 1\n",
    "## Neuronale Netze - Beispielprogramm\n",
    "### Implementierung eines neuronalen Netzes ohne Hidden Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Preparations\n",
    "#### 1.1 - Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Display plots inline and change default figure size\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 - Generating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dataset and plot it\n",
    "n          = 1000\n",
    "data_seed  = 1337\n",
    "split_seed = 42\n",
    "test_size  = 0.25\n",
    "\n",
    "np.random.seed(data_seed)\n",
    "X, y = sklearn.datasets.make_moons(n, noise=0.25)\n",
    "plt.scatter(X[:,0], X[:,1], s=40, c=y, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 - Split the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                    y, \n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=split_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train[:,0], X_train[:,1], s=40, c=y_train, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Validation Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test[:,0], X_test[:,1], s=40, c=y_test, cmap=plt.cm.Spectral);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Implementierung\n",
    "#### 2.1 - Aktivierungsfunktion (activation function)\n",
    "Als Aktivierungsfunktion des Output-Layers wird die logistische Funktion \n",
    "\n",
    "$\\displaystyle \\sigma\\colon \\,\\mathbb{R}\\to \\left(0,1\\right), \\;z \\mapsto\\frac{1}{1+\\exp(-z)}$ \n",
    "\n",
    "verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z, derivation = False):\n",
    "    if derivation:\n",
    "        return sigmoid(z)*(1-sigmoid(z))\n",
    "    else:\n",
    "        return 1/(1+np.exp(-z))\n",
    "\n",
    "value = 1\n",
    "print(sigmoid(value))\n",
    "print(sigmoid(value,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Kostenfunktion (cost function)\n",
    "Als Kostenfunktion wird folgende quadratische Fehlerfunktion verwendet:\n",
    "\n",
    "$\\displaystyle E(y,\\widehat{\\pi}) = \\frac{1}{2}\\sum_{k=1}^{n}\\left(y_k-\\widehat{\\pi_k}\\right)^2$\n",
    "\n",
    "wobei $y=\\left(y_1,\\dots,y_n\\right)^T$ und $\\widehat{\\pi}=\\left(\\widehat{\\pi_1},\\dots,\\widehat{\\pi_n}\\right)^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booklet teil 2\n",
    "hier startet unsere Implementierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate=0.1):\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        #random initialization of weights\n",
    "        rand_range = 0.05\n",
    "        np.random.seed(None)\n",
    "        \n",
    "        self.W_input_hidden = np.random.uniform(low = -rand_range,\n",
    "                                                high = rand_range,\n",
    "                                                size= (self.hidden_nodes, self.input_nodes))\n",
    "        \n",
    "        self.W_hidden_output = np.random.uniform(low = -rand_range,\n",
    "                                                 high = rand_range,\n",
    "                                                 size= (self.output_nodes, self.hidden_nodes))\n",
    "    \n",
    "    def calculate_loss(self, inputs,labels):\n",
    "        predictions = self.predict(inputs)\n",
    "\n",
    "        # Berechnung des Kostenfunktionswertes\n",
    "        cost = np.power(predictions-labels,2)\n",
    "        cost = np.sum(cost)/2\n",
    "\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    def fit(self, inputs, label):\n",
    "        label = np.array(label, ndmin=2).T\n",
    "        output, hidden_outputs = self.predict(inputs, backprop=True)\n",
    "        \n",
    "        #Update weights from hidden to output layer\n",
    "        output_error = output - label  #getting this from the derived squared error loss function\n",
    "        #note that \"output_error*outputs*(1.0-outputs)\" comes from the derivate of sigmoid acrivation.\n",
    "        gradient_weights_hidden_output = np.dot((output_error*output*(1.0-output)), np.transpose(hidden_outputs))\n",
    "        self.W_hidden_output += -self.learning_rate * gradient_weights_hidden_output\n",
    "        \n",
    "        #Update weights from input to hidden layer\n",
    "        #first \"propagate\" the errors back to the hidden layer.\n",
    "        hidden_errors = np.dot(self.W_hidden_output.T, (output_error*output*(1.0-output)))\n",
    "        \n",
    "        #this step is the same as from the previous update. just one layer closer to the input.\n",
    "        gradient_weights_input_hidden = np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)),\n",
    "                                              np.array(inputs, ndmin=2))\n",
    "        self.W_input_hidden += -self.learning_rate * gradient_weights_input_hidden\n",
    "        \n",
    "        pass\n",
    "        \n",
    "    def predict(self, inputs, backprop=False):\n",
    "        \n",
    "        inputs = np.array(inputs, ndmin=2).T\n",
    "        \n",
    "        # feedforward from input layer to hidden layer\n",
    "        hidden_inputs = np.dot(self.W_input_hidden, inputs)\n",
    "        hidden_outputs = sigmoid(hidden_inputs)\n",
    "        \n",
    "        #feedforward from hidden layer to output layer\n",
    "        output_inputs = np.dot(self.W_hidden_output, hidden_outputs)\n",
    "        output_outputs = sigmoid(output_inputs)\n",
    "        \n",
    "        if backprop == True:\n",
    "            #returning hidden outputs which we need in case of backpropagation\n",
    "            return (output_outputs, hidden_outputs)\n",
    "        \n",
    "        return output_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create nn\n",
    "nn = NeuralNet(input_nodes=2, hidden_nodes=2, output_nodes=1, learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train nn \n",
    "for sample, sample_label, in zip(X_train, y_train):\n",
    "    nn.fit(sample, sample_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.calculate_loss(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the accuracy\n",
    "predictions = nn.predict(X_test)\n",
    "rounded_predictions = np.where(predictions > 0.5,1,0)\n",
    "rounded_predictions[0]\n",
    "\n",
    "accuracy = (rounded_predictions[0] == y_test).mean()\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Anpassung des Modells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Auswertung der Ergebnisse\n",
    "#### 3.1 - Grafische Veranschaulichung des Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot a decision boundary.\n",
    "def plot_decision_boundary(pred_func,X,y):\n",
    "    # Set min and max values and give it some padding\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    h = 0.01\n",
    "    \n",
    "    # Generate a grid of points with distance h between them\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict the function value for the whole gid\n",
    "    Z = pred_func(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot the contour and training examples\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.Spectral)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.1 - Gesamter Datensatz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict(model, x),X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.2 - Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict(model, x),X_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1.3 - Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: predict(model, x),X_test,y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "# Training Data\n",
    "probs_train = predict(model,X_train,proba=True)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, probs_train)\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Validation Data\n",
    "probs_test = predict(model,X_test,proba=True)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, probs_test)\n",
    "auc_test = auc(fpr_test, tpr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves + AUC values\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'b', label = 'Train  (AUC = %0.4f)' % auc_train)\n",
    "plt.plot(fpr_test,  tpr_test,  'g', label = 'Test   (AUC = %0.4f)' % auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Confusion Matrix\n",
    "##### 3.3.1 Trainingsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu_train = pd.Series(y_train, name='Actual')\n",
    "y_pred_train = pd.Series(predict(model,X_train)[:,0], name='Predicted')\n",
    "df_conf_train = pd.crosstab(y_actu_train, y_pred_train)\n",
    "print(df_conf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.3.1 Validierungsdaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_actu_test = pd.Series(y_test, name='Actual')\n",
    "y_pred_test = pd.Series(predict(model,X_test)[:,0], name='Predicted')\n",
    "df_conf_test = pd.crosstab(y_actu_test, y_pred_test)\n",
    "print(df_conf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_actu_train,y_pred_train)\n",
    "acc_test  = accuracy_score(y_actu_test, y_pred_test)\n",
    "print(\"-- Accuracy --\")\n",
    "print(\"Train:\\t{:2.2f}%\".format(acc_train*100))\n",
    "print(\"Test:\\t{:2.2f}%\".format(acc_test*100))\n",
    "print(\"\\n-- Misclassification Rate --\")\n",
    "print(\"Train:\\t{:2.2f}%\".format((1-acc_train)*100))\n",
    "print(\"Test:\\t{:2.2f}%\".format((1-acc_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Deep Learning Library Keras\n",
    "#### 4.1 Aufbau und Training des Modells\n",
    "Mithilfe von Keras können einzelne Layer sequentiell dem Modell hinzugefügt werden. Siehe [Getting started with the Keras Sequential model](https://keras.io/getting-started/sequential-model-guide/) und [The Sequential model API](https://keras.io/models/sequential/)\n",
    "Unter anderem sind dabei folgende Punkte zu beachten:\n",
    "* Beim ersten Layer muss mit **input_dim**, die Anzahl der Features (abhängigen Merkmalsvariablen) angegeben werden\n",
    "* Beim letzten Layer muss die Anzahl der Neuronen mit der Anzahl der Targets übereinstimmen\n",
    "\n",
    "Alle nötigen Informationen zum Umgang mit Keras können Sie auf der offiziellen Seite von Keras finden:\n",
    "**[Keras: The Python Deep Learning library](https://keras.io)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.initializers import RandomUniform,Constant\n",
    "\n",
    "#START A NEW TF SESSION - Für Reproduzierbarkeit notwendig\n",
    "import tensorflow as tf\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(1337)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph())\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1  Aufbau des Modells\n",
    "Zunächst muss mit $\\texttt{Sequential}$ ein Modell-Objekt erzeugt werden. Anschließend können mit der Methode $\\texttt{add}$ Layer hinzugefügt werden. \n",
    "- Mögliche Einstellungen der einfachen Layer: [Core Layers](https://keras.io/layers/core/)\n",
    "- [Usage of activations](https://keras.io/activations/)\n",
    "- [Usage of regularizers](https://keras.io/regularizers/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "keras_model = Sequential()\n",
    "\n",
    "# Add Layers\n",
    "keras_model.add(Dense(1,\n",
    "                activation='sigmoid',\n",
    "                input_dim=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2  Optionen für die Modellanpassung\n",
    "Anschließend müssen mit der compile-Methode verschiedene Einstellungen für die Modellanpassung getroffen werden.\n",
    "- **optimizer:** Optimierungsalgorithmus ([Usage of optimizers](https://keras.io/optimizers/))\n",
    "- **loss:** Auswahl der gewünschten Zielfunktion nach der das Modell optimiert werden soll ([Usage of loss functions](https://keras.io/losses/))\n",
    "- **metrics:** Metriken die werden der Modellanpassung berechnet werden sollen\n",
    "- Weitere Parameter sind unter [The Sequential model API\n",
    "](https://keras.io/models/sequential/) zu finden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Options\n",
    "keras_model.compile(optimizer='rmsprop',\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Übersicht des Modells\n",
    "print(keras_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.3 Anpassung des Modells\n",
    "Durch die Methode $\\texttt{fit}$ kann das Modell schließlich trainiert werden. Siehe hierzu wieder: \n",
    "[The Sequential model API](https://keras.io/models/sequential/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting of the model\n",
    "model_history = keras_model.fit(X_train, #Idealerweise sollten die Daten standardisiert werden\n",
    "                                y_train, \n",
    "                                epochs          = 250,\n",
    "                                validation_data = (X_test, y_test),\n",
    "                                verbose         = 2,                     # 0 = kein Output \n",
    "                                batch_size      = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Auswertungen\n",
    "##### 4.2.1 Zielfunktion und Genauigkeit im Verlauf der Epochen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kostenfunktion\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(model_history.epoch,\n",
    "         model_history.history['loss'], \n",
    "         'b', \n",
    "         label = \"Training\")\n",
    "plt.plot(model_history.epoch,\n",
    "         model_history.history['val_loss'],\n",
    "         'g',\n",
    "         label = \"Validation\")\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.title('Kostenfunktion');\n",
    "\n",
    "# Genauigkeit\n",
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(model_history.epoch,\n",
    "         model_history.history['accuracy'],\n",
    "        'b',\n",
    "        label = \"Training\")\n",
    "plt.plot(model_history.epoch,\n",
    "         model_history.history['val_accuracy'],\n",
    "        'g',\n",
    "        label= \"Validation\")\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.title('Genauigkeit');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.2 Grafische Darstellung des Modells\n",
    "\n",
    "**Gesamter Datensatz**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: keras_model.predict_classes(x),X,y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trainingsdaten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: keras_model.predict_classes(x),X_train,y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validierungsdaten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_decision_boundary(lambda x: keras_model.predict_classes(x),X_test,y_test);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.4 ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "# Training Data\n",
    "probs_train = keras_model.predict_proba(X_train)\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, probs_train[:,0])\n",
    "auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "# Validation Data\n",
    "probs_test = keras_model.predict_proba(X_test)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, probs_test[:,0])\n",
    "auc_test = auc(fpr_test, tpr_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves + AUC values\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr_train, tpr_train, 'b', label = 'Train  (AUC = %0.4f)' % auc_train)\n",
    "plt.plot(fpr_test,  tpr_test,  'g', label = 'Test   (AUC = %0.4f)' % auc_test)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.5 Confusion Matrix\n",
    "**Trainingsdaten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_actu_train = pd.Series(y_train, name='Actual')\n",
    "y_pred_train = pd.Series(keras_model.predict_classes(X_train)[:,0], name='Predicted')\n",
    "df_conf_train = pd.crosstab(y_actu_train, y_pred_train)\n",
    "print(df_conf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Validierungsdaten**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_actu_test = pd.Series(y_test, name='Actual')\n",
    "y_pred_test = pd.Series(keras_model.predict_classes(X_test)[:,0], name='Predicted')\n",
    "df_conf_test = pd.crosstab(y_actu_test, y_pred_test)\n",
    "print(df_conf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.2.6 Metriken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train = accuracy_score(y_actu_train,y_pred_train)\n",
    "acc_test  = accuracy_score(y_actu_test, y_pred_test)\n",
    "print(\"-- Accuracy --\")\n",
    "print(\"Train:\\t{:2.2f}%\".format(acc_train*100))\n",
    "print(\"Test:\\t{:2.2f}%\".format(acc_test*100))\n",
    "print(\"\\n-- Misclassification Rate --\")\n",
    "print(\"Train:\\t{:2.2f}%\".format((1-acc_train)*100))\n",
    "print(\"Test:\\t{:2.2f}%\".format((1-acc_test)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLOSE TF SESSION\n",
    "tf.compat.v1.keras.backend.clear_session()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
