\pagebreak
\section*{Zusammenfassung}
\addcontentsline{toc}{section}{Zusammenfassung}
Die vorliegende Arbeit verfolgt das Ziel der theoretischen und praktischen Vorstellung vier unterschiedlicher Data Mining Techniken. Zudem liegt ein Fokus auf der Vorbereitung, die die Datenbereinigung und die Parameterwahl umfasst. Es wird ein Datensatz betrachtet, für den verschiedene Klassifizierer erstellt werden, welche anhand mehrere Merkmale den Wert einer binären Zielvariable bestimmt.\\
\noindent \hspace*{7mm}
Im ersten Teil wird das Konzept der Entscheidungsbäume vorgestellt. Es wird auf verschiedene Methoden eingegangen, durch die ein Entscheidungsbaum gekürzt werden kann, um seine Performance zu verbessern. Dabei ist ersichtlich geworden, dass im Bereich des \emph{pre-prunings} weiche Abbruchbedingungen in einer besseren Performance resultieren als harte, da sie den Fokus auf Kriterien der einzelnen Knoten legen (z.B. Minimale Reduktion der Ungenauigkeit), statt den Baum z.B. an einer bestimmten Größe zu kappen (Maximale Tiefe). Zudem regulieren sie die Grenze zwischen Über- und Unteranpassung besser. Der beste modellierte Entscheidungsbaum erreicht eine Genauigkeit von 85,07 \% durch ein \emph{Cost-Complexity Pruning} mit dem Parameter $\alpha = 0.00025$\\
\noindent \hspace*{7mm}
Der zweiten Teil beinhaltet das Konzept der neuronalen Netze. Es wird kurz auf das Konzept der Backpropagation eingegangen und wie diese an einem konkreten Beispiel implementiert werden kann. Auch in diesem Kapitel werden einige Parameter und deren Variationen besprochen. Hierbei konnten nicht alle theoretischen Annahmen bestätigt werden. Zum Beispiel konnte bei einer hohen Anzahl an Neuronen keine Überanpassung des Netzwerks festgestellt werden. Jedoch wurde bei einer zu kleinen Lernrate beobachtet, dass das Netz nur sehr langsam optimiert wird. Das beste modellierte neuronale Netz erreicht eine Genauigkeit von 85,11\% bei 4 Hidden Layern mit je 3 Neuronen.\\
\noindent \hspace*{7mm}
Im dritten Teil werden verschiedene Ensemblemethoden an einem Entscheidungsbaum angewandt. Dieses Kapitel legt den Fokus auf das Verfahren der Suche nach optimalen Parameter-Kombinationen, sodass hier die meisten Parameter betrachtet werden. Die Suche wird in univariat und einen multivariat aufgeteilt. Das beste Ergebnis wird mit dem \emph{AdaBoost} Verfahren erzielt und erreicht eine Genauigkeit von 86,64\%.\\
\noindent \hspace*{7mm}
Der letzte Teil dieser Arbeit stellt das Konzept der \emph{Support Vector Machines} vor. Es wird eine kurze theoretische Beschreibung gegeben und auf die unterschiedlichen Parameter und deren Einflüsse auf das Modell und dessen Komplexität eingegangen. Zudem wird der Einsatz verschiedener \emph{Kernel} betrachtet, die jeweils eigene Parameter benutzen. Bei den Support Vector Machines wurde das beste Ergebnis mit dem \emph{Radial Basis Function} Kernel, einem C-Wert von 10 mit dem Default-Wert für $\gamma$ erzeugt. Das erzeugte Modell ht eine Genauigkeit von 86,198\%.\\
\noindent \hspace*{7mm}
Anhand der Ergebnisse ist ersichtlich, dass keine der Methoden deutlich bessere Ergebnisse erzielt als die anderen. Zudem wurde ersichtlich, dass die einzelnen Parameter der Methoden starken Einfluss auf das jeweilige Modell haben.