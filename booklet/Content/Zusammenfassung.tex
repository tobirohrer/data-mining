\pagebreak
\section*{Zusammenfassung}
\addcontentsline{toc}{section}{Zusammenfassung}
Die vorliegende Arbeit verfolgt das Ziel der theoretischen und praktischen Vorstellung vier unterschiedlicher Klassifizierungsverfahren. Die Daten zum Trainieren und Testen der Verfahren wurden in der Aufgabenstellung gestellt und stellen Wetterdaten dar. Das Ziel ist es anhand der Wetterdaten vom Vortag vorauszusagen, ob es am Folgetag regnet oder nicht.

\noindent \hspace*{7mm}
Im ersten Teil wird die Vorverarbeitung der Daten, sowie das Konzept der Entscheidungsbäume vorgestellt. In der Vorbereitung der Daten hat sich die Bereinigung und Imputierung von fehlenden Werten als besonders Wichtig herausgestellt. Verfahren zur Beseitigung von Ausreißern, sowie zur Vorselektion von Eingabevariablen wurden getestet, haben aber zu keiner Verbesserung im Klassifizierungsergebnis beigetragen. Außerdem wurde eine Normalisierung der Werte vorgenommen, was für eine Verarbeitung durch eine Support Vektor Machine (SVM) oder neuronales Netz empfehlenswert ist. Verschiedene Methoden um einen Entscheidungsbaum vor einer Überanpassung an Trainingsdaten zu bewahren und seine Klassifizierungsgenauigkeit auf Testdaten zu verbessern wurden ebenfalls im ersten Teil untersucht.

\noindent \hspace*{7mm}
Der zweiten Teil beinhaltet das Konzept der neuronalen Netze. Backpropagation wurde anhand eines einfachen neuronalen Netzes mit einem Hidden Layer mathematisch hergeleitet und in Python implementiert. Die Auswirkung von einzelnen Hyperparametern wurde anhand dieses einfachen Beispiels diskutiert und veranschaulicht. Darüber hinaus wurde ein weiteres neuronales Netzwerk mit Hilfe der \emph{TensorFlow}-Bibliothek implementiert, um eine Klassifizierung der Wetterdaten durchzuführen. Durch den Durchlauf einer \emph{Random Search} konnte eine Hyperparameter-Kombination gefunden werden, mit der eine Klassifizierungsgenauigkeit von 85,11\% erreicht werden konnte.

\noindent \hspace*{7mm}
Im dritten Teil wird das Ergebnis einer Hyperparameter-Optimierung mittels des \emph{Grid Search} Verfahrens beschrieben. Verschiedene Ensemblemethoden und ein Entscheidungsbaum wurden hierbei einzeln optimiert. Durch eine zweistufige Grid Search mit univariaten Parameterräumen konnten die besten Klassifizierungsgenauigkeiten erreicht werden. Das beste Ergebnis konnte mit dem \emph{AdaBoost} Verfahren erzielt werden und erreicht eine Klassifizierungsgenauigkeit von 86,64\%.\\
\noindent \hspace*{7mm}
Der letzte Teil dieser Arbeit stellt das Konzept der \emph{Support Vector Machines} vor. Es wird eine kurze theoretische Beschreibung gegeben und auf die unterschiedlichen Parameter und deren Einflüsse auf das Modell und dessen Komplexität eingegangen. Zudem wird der Einsatz verschiedener \emph{Kernel} betrachtet, die jeweils eigene Parameter benutzen. Bei den Support Vector Machines wurde das beste Ergebnis mit dem \emph{Radial Basis Function} Kernel, einem C-Wert von 10 mit dem Default-Wert für $\gamma$ erzeugt. Das erzeugte Modell ht eine Genauigkeit von 86,198\%.\\
\noindent \hspace*{7mm}

Anhand der Ergebnisse ist ersichtlich, dass mit Hilfe des AdaBoost Verfahrens die besten Klassifizierungsergebnise erzielt werden konnten. Jedoch hat das Training des AdaBoost-Klassifizierers im Vergleich zum Random Forest oder Bagging-Klassifizierers erheblich länger gedauert. 