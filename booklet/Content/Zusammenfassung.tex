\pagebreak
\section*{Zusammenfassung}
\addcontentsline{toc}{section}{Zusammenfassung}
Die vorliegende Arbeit verfolgt das Ziel der theoretischen und praktischen Vorstellung vier unterschiedlicher Klassifizierungsverfahren. Die Daten zum Trainieren und Testen der Verfahren wurden in der Aufgabenstellung gestellt und stellen Wetterdaten dar. Das Ziel ist es, anhand der Wetterdaten vom Vortag vorauszusagen, ob es am Folgetag regnet oder nicht.

\noindent \hspace*{7mm}
Im ersten Teil wird die Vorverarbeitung der Daten, sowie das Konzept der Entscheidungsbäume vorgestellt. In der Vorverarbeitung der Daten hat sich die Bereinigung und Imputierung von fehlenden Werten als besonders wichtig herausgestellt. Verfahren zur Beseitigung von Ausreißern, sowie zur Vorselektion von Eingabevariablen wurden getestet, haben aber zu keiner Verbesserung im Klassifizierungsergebnis beigetragen. Außerdem wurde eine Normalisierung der Werte vorgenommen, was für eine Verarbeitung durch eine Support Vektor Machine (SVM) oder neuronales Netz empfehlenswert ist. Verschiedene Methoden, um einen Entscheidungsbaum vor einer Überanpassung an Trainingsdaten zu bewahren und seine Klassifizierungsgenauigkeit auf Testdaten zu verbessern, wurden ebenfalls im ersten Teil untersucht.  Den Baum auf seine volle Größe mit den Standard-Einstellungen wachsen zu lassen und ihn im Nachgang via \emph{Cost-Complexity-Pruning} zu beschneiden, hat sich als beste Methode herausgestellt. Dadurch konnte eine Klassifizierungsgenauigkeit von 85,07\% erreicht werden.

\noindent \hspace*{7mm}
Der zweite Teil beinhaltet das Konzept der neuronalen Netze. Backpropagation wurde anhand eines einfachen neuronalen Netzes mit einem Hidden Layer mathematisch hergeleitet und in Python implementiert. Die Auswirkung von einzelnen Hyperparametern wurde anhand dieses einfachen Beispiels diskutiert und veranschaulicht. Darüber hinaus wurde ein weiteres neuronales Netzwerk mit Hilfe der \emph{TensorFlow}-Bibliothek implementiert, um eine Klassifizierung der Wetterdaten durchzuführen. Durch den Durchlauf einer \emph{Random Search} konnte eine Hyperparameter-Kombination gefunden werden, mit der eine Klassifizierungsgenauigkeit von 85,11\% erreicht werden konnte.

\noindent \hspace*{7mm}
Im dritten Teil wurden verschiedene Ensemblemethoden implementiert und mit Hilfe einer \emph{Grid Search} optimiert. Durch eine zweistufige Grid Search mit univariaten Parameterräumen konnten die besten Hyperparameter-Kombinationen gefunden werden. Das beste Ergebnis konnte mit dem \emph{AdaBoost}-Verfahren erzielt werden und erreicht eine Klassifizierungsgenauigkeit von 86,64\%.\\
\noindent \hspace*{7mm}
Der letzte Teil dieser Arbeit stellt das Konzept der \emph{Support Vector Machines} vor. Es wird eine kurze theoretische Beschreibung gegeben und auf die unterschiedlichen Parameter und deren Einflüsse auf das Modell und dessen Komplexität eingegangen. Zudem wird der Einsatz verschiedener \emph{Kernel} betrachtet, die jeweils eigene Parameter benutzen. Bei den Support Vector Machines wurde das beste Ergebnis mit dem \emph{Radial Basis Function} Kernel, einem C-Wert von 10 mit dem Default-Wert für $\gamma$ erzeugt. Das erzeugte Modell ht eine Genauigkeit von 86,198\%.\\
\noindent \hspace*{7mm}

Anhand der Ergebnisse ist ersichtlich, dass mit Hilfe des  \emph{AdaBoost} Verfahrens die besten Klassifizierungsergebnise erzielt werden konnten. Jedoch hat das Training des  \emph{AdaBoost}-Klassifizierers im Vergleich zum  \emph{Random Forest} oder  \emph{Bagging}-Klassifizierers erheblich länger gedauert. Mit Hilfe des  \emph{Random Forest} konnten mit 86,34\% die zweitbesten Ergebnisse erzielt werden. Der  \emph{Random Forest} hat den Vorteil, dass das Training der Basisklassifizierer unabhängig voneinander erfolgt und daher parallelisierbar ist. Mit Hilfe eines einzelnen Entscheidungsbaums konnten dagegen nur 85,07\% der Daten richtig klassifiziert werden. Der Entscheidungsbaum hat jedoch den Vorteil, dass er leicht zu interpretieren ist. Durch den Einsatz eines neuronalen Netzes konnten nur 85,11\% Klassifizierungsgenauigkeit erreicht werden. Hierbei ist zu erwarten, dass bei der Ausweitung der angewandten \emph{Random Search} bessere Ergebnisse erzielt werden können.    